#!/usr/bin/env python3
"""
Evaluates on ACTUAL BridgeData V2 task categories available in data/scripted_raw/
This is a valid SOTA replication using the real training data distribution
"""

import os
import sys
import time
import json
import numpy as np
import torch
import pickle
from pathlib import Path
from PIL import Image
from tqdm import tqdm
from transformers import AutoProcessor, AutoModelForVision2Seq
from datetime import datetime
import argparse
from typing import List, Dict, Tuple, Any
import random

# Set environment for Apple Silicon stability
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

class SOTAOpenVLAReplicator:
    """Replicates OpenVLA evaluation on actual BridgeData V2 task categories"""
    
    def __init__(self, model_path: str = "openvla/openvla-7b", device: str = "auto"):
        self.model_path = model_path
        self.device = self._setup_device(device)
        self.processor = None
        self.model = None
        self.results = []
        
        # These match the folder prefixes in data/scripted_raw/
        self.bridgedata_tasks = {
            "pnp_rigid_objects": "pick up the object and place it in the bowl",
            "pnp_soft_toys": "pick up the soft toy and place it in the bowl",
            "pnp_utensils": "pick up the utensil and place it in the target location",
            "pnp_many_objects_in_env": "pick up the object and place it in the bowl",
            "pnp_objects": "pick up the object and place it in the bowl",
            "sweep": "sweep to dustpan",
        }
        
        # Evaluation metrics tracking
        self.task_results = {task: [] for task in self.bridgedata_tasks.keys()}
        self.global_metrics = {
            'total_rollouts': 0,
            'successful_rollouts': 0,
            'task_success_rates': {},
            'overall_success_rate': 0.0,
            'average_mae': 0.0,
            'average_mse': 0.0
        }
    
    def _setup_device(self, device: str) -> str:
        """Setup device with CPU preference for stability"""
        if device == "auto":
            device = "cpu"
            print("ðŸ”§ Using CPU for maximum stability and compatibility")
        elif device == "mps" and torch.backends.mps.is_available():
            device = "mps"
            print("ðŸŽ Using Apple Silicon MPS with CPU fallback")
        elif device == "cuda" and torch.cuda.is_available():
            device = "cuda"
            print("ðŸš€ Using CUDA GPU")
        else:
            device = "cpu"
            print("ðŸ”§ Using CPU for maximum stability")
        
        print(f"ðŸ”§ Using device: {device}")
        return device
    
    def load_model(self):
        """Load OpenVLA model"""
        print(f"ðŸš€ Loading OpenVLA model: {self.model_path}")
        
        try:
            self.processor = AutoProcessor.from_pretrained(self.model_path, trust_remote_code=True)
            self.model = AutoModelForVision2Seq.from_pretrained(
                self.model_path,
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            ).to("cpu")
            
            self.model.eval()
            
            print(f"âœ… Model loaded successfully on {self.device}")
            return True
            
        except Exception as e:
            print(f"âŒ Model loading failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def load_real_bridgedata_samples(self, task_prefix: str, max_trials: int = 10) -> List[Dict[str, Any]]:
        """
        Load REAL BridgeData V2 samples from data/scripted_raw/ directory
        Args:
            task_prefix: e.g., "pnp_rigid_objects", "sweep"
            max_trials: Maximum number of samples to load
        """
        print(f"ðŸ“Š Loading real BridgeData V2 samples for {task_prefix}...")
        
        samples = []
        data_dir = Path("data/scripted_raw")
        
        if not data_dir.exists():
            print(f"âš ï¸  Data directory not found: {data_dir}")
            return samples
        
        # Get instruction for this task
        instruction = self.bridgedata_tasks.get(task_prefix, "complete the manipulation task")
        
        # Find all folders that match this task prefix
        # e.g., "pnp_rigid_objects" matches "2022-12-08_pnp_rigid_objects"
        matching_folders = []
        for folder in data_dir.iterdir():
            if folder.is_dir() and task_prefix in folder.name:
                matching_folders.append(folder)
        
        print(f"   Found {len(matching_folders)} folders matching '{task_prefix}'")
        
        # Walk through matching folders to find trajectories
        for task_folder in matching_folders:
            if len(samples) >= max_trials:
                break
                
            for root, dirs, files in os.walk(task_folder):
                if len(samples) >= max_trials:
                    break
                    
                if 'policy_out.pkl' in files:
                    traj_dir = Path(root)
                    img_dir = traj_dir / "images0"
                    
                    if not img_dir.exists():
                        continue
                        
                    img_files = list(img_dir.glob("im_*.jpg"))
                    if len(img_files) < 2:
                        continue
                    
                    try:
                        with open(traj_dir / "policy_out.pkl", 'rb') as f:
                            actions = pickle.load(f)
                        
                        # Handle different action formats
                        if isinstance(actions, dict):
                            action_data = actions.get('actions', actions.get('action', list(actions.values())[0]))
                            if isinstance(action_data, (list, tuple)) and len(action_data) > 0:
                                gt_actions = np.array(action_data)
                            elif isinstance(action_data, np.ndarray):
                                gt_actions = action_data
                            else:
                                continue
                        elif isinstance(actions, (list, tuple)):
                            gt_actions = np.array(actions)
                        elif isinstance(actions, np.ndarray):
                            gt_actions = actions
                        else:
                            continue

                        # Normalize to (T, 7) array
                        try:
                            if isinstance(gt_actions, np.ndarray) and gt_actions.dtype == object and gt_actions.ndim == 1 and gt_actions.size > 0:
                                if isinstance(gt_actions[0], dict):
                                    extracted = []
                                    for step in gt_actions:
                                        if not isinstance(step, dict):
                                            break
                                        step_action = step.get('actions', step.get('action', None))
                                        if step_action is None:
                                            break
                                        step_vec = np.array(step_action).flatten()
                                        if step_vec.size < 7:
                                            step_vec = np.pad(step_vec, (0, 7 - step_vec.size), 'constant')
                                        extracted.append(step_vec[:7])
                                    if extracted:
                                        gt_actions = np.stack(extracted, axis=0)
                        except Exception:
                            pass
                        
                        # Ensure we have at least 2D array
                        if gt_actions.ndim == 1:
                            gt_actions = gt_actions.reshape(1, -1)
                        
                        # Pad to 7 dimensions if needed
                        if gt_actions.shape[-1] < 7:
                            pad_width = ((0, 0), (0, 7 - gt_actions.shape[-1]))
                            gt_actions = np.pad(gt_actions, pad_width, 'constant')
                        
                        gt_actions = gt_actions[:, :7]  # Take only first 7 dimensions
                        
                        print(f"âœ… Found trajectory: {traj_dir.name}")
                        print(f"   Actions shape: {gt_actions.shape}")
                        
                        samples.append({
                            'traj_dir': traj_dir,
                            'img_dir': img_dir,
                            'img_files': sorted(img_files),
                            'gt_actions': gt_actions,
                            'instruction': instruction,
                            'task': task_prefix
                        })
                        
                    except Exception as e:
                        print(f"âš ï¸  Error loading {traj_dir.name}: {e}")
                        continue
        
        print(f"âœ… Loaded {len(samples)} real samples for {task_prefix}")
        if len(samples) < max_trials:
            print(f"âš ï¸  Warning: Only {len(samples)} samples found (requested {max_trials})")
        
        return samples
    
    def predict_action(self, image: Image.Image, instruction: str) -> np.ndarray:
        """Predict action using OpenVLA model"""
        try:
            inputs = self.processor(instruction, image).to("cpu", dtype=torch.float32)
            
            with torch.no_grad():
                action = self.model.predict_action(**inputs, unnorm_key="bridge_orig", do_sample=False)
            
            # Convert to numpy
            if isinstance(action, torch.Tensor):
                action = action.cpu().numpy()
            
            # Ensure 1D array of length 7
            action = np.array(action).flatten()
            if action.size < 7:
                action = np.pad(action, (0, 7 - action.size), 'constant')
            
            return action[:7]
            
        except Exception as e:
            print(f"âš ï¸  Prediction error: {e}")
            return np.zeros(7)
    
    def evaluate_rollout(self, sample: Dict[str, Any], max_timesteps: int = 2) -> Dict[str, Any]:
        """
        Evaluate a single rollout (trajectory)
        
        Args:
            sample: Sample dictionary with trajectory data
            max_timesteps: Number of timesteps to evaluate per rollout
        """
        traj_dir = sample['traj_dir']
        img_files = sample['img_files']
        gt_actions = sample['gt_actions']
        instruction = sample['instruction']
        
        print(f"   Processing {max_timesteps} timesteps (max {min(max_timesteps, len(img_files))})...")
        
        predictions = []
        ground_truths = []
        step_results = []
        
        # Evaluate up to max_timesteps
        num_steps = min(max_timesteps, len(img_files), len(gt_actions))
        
        for t in range(num_steps):
            # Load image
            img_path = img_files[t]
            image = Image.open(img_path).convert('RGB')
            
            # Get prediction
            pred_action = self.predict_action(image, instruction)
            gt_action = gt_actions[t]
            
            # Calculate errors
            mae = np.mean(np.abs(pred_action - gt_action))
            mse = np.mean((pred_action - gt_action) ** 2)
            
            # Position error (first 3 dimensions: x, y, z)
            pos_error = np.linalg.norm(pred_action[:3] - gt_action[:3])
            
            # Rotation error (dimensions 3-6: roll, pitch, yaw)
            rot_error = np.linalg.norm(pred_action[3:6] - gt_action[3:6])
            
            # Gripper error (dimension 6)
            grip_error = np.abs(pred_action[6] - gt_action[6])
            
            # Success criteria (using reasonable thresholds)
            success_threshold = 0.2  # 20cm/20deg tolerance
            pos_success = pos_error < success_threshold
            rot_success = rot_error < success_threshold
            grip_success = grip_error < success_threshold
            
            # Overall step success (all components must succeed)
            step_success = pos_success and rot_success and grip_success
            
            predictions.append(pred_action)
            ground_truths.append(gt_action)
            step_results.append({
                'timestep': t,
                'mae': float(mae),
                'mse': float(mse),
                'pos_error': float(pos_error),
                'rot_error': float(rot_error),
                'grip_error': float(grip_error),
                'success': step_success
            })
        
        # Rollout-level metrics
        rollout_mae = np.mean([s['mae'] for s in step_results])
        rollout_mse = np.mean([s['mse'] for s in step_results])
        successful_steps = sum([s['success'] for s in step_results])
        rollout_success = successful_steps == num_steps  # All steps must succeed
        
        print(f"   âœ… Rollout complete: {successful_steps}/{num_steps} steps successful")
        
        return {
            'traj_name': traj_dir.name,
            'instruction': instruction,
            'num_steps': num_steps,
            'rollout_success': rollout_success,
            'successful_steps': successful_steps,
            'rollout_mae': float(rollout_mae),
            'rollout_mse': float(rollout_mse),
            'step_results': step_results,
            'predictions': [p.tolist() for p in predictions],
            'ground_truths': [g.tolist() for g in ground_truths]
        }
    
    def run_evaluation(self, trials_per_task: int = 10, timesteps_per_trial: int = 2):
        """
        Run full SOTA evaluation on all available BridgeData V2 tasks
        
        Args:
            trials_per_task: Number of rollouts per task (default: 10, matching OpenVLA paper)
            timesteps_per_trial: Number of timesteps per rollout (default: 2)
        """
        print("âœ… Model loaded successfully on cpu")
        print(f"ðŸ”§ Using {timesteps_per_trial} timesteps per trial")
        
        num_tasks = len(self.bridgedata_tasks)
        total_predictions = num_tasks * trials_per_task * timesteps_per_trial
        print(f"ðŸŽ¯ Total evaluation: {num_tasks} Ã— {trials_per_task} Ã— {timesteps_per_trial} = {total_predictions} predictions")
        
        print("ðŸš€ STARTING SOTA OPENVLA REPLICATION WITH REAL DATA")
        print("=" * 80)
        print(f"ðŸ“Š Evaluation Plan: {num_tasks} tasks Ã— {trials_per_task} samples Ã— {timesteps_per_trial} timesteps")
        print(f"ðŸŽ¯ Total Predictions: {num_tasks} Ã— {trials_per_task} Ã— {timesteps_per_trial} = {total_predictions}")
        print(f"ðŸŽ¯ Tasks: {', '.join(self.bridgedata_tasks.keys())}")
        print(f"ðŸ“‚ Data Source: data/scripted_raw/ (REAL BridgeData V2)")
        print("=" * 80)
        
        all_results = {}
        task_num = 1
        
        for task_name, instruction in self.bridgedata_tasks.items():
            print(f"\nðŸ“‹ TASK {task_num}/{num_tasks}: {task_name.upper()}")
            print("-" * 60)
            
            # Load samples for this task
            samples = self.load_real_bridgedata_samples(task_name, max_trials=trials_per_task)
            
            if len(samples) == 0:
                print(f"âš ï¸  No samples found for {task_name}. Skipping...")
                task_num += 1
                continue
            
            # Evaluate each sample
            task_results = []
            for sample in samples:
                print(f"ðŸŽ¯ Running {task_name} - Sample {sample['traj_dir'].name}")
                result = self.evaluate_rollout(sample, max_timesteps=timesteps_per_trial)
                task_results.append(result)
            
            # Calculate task-level metrics
            task_success_rate = np.mean([r['rollout_success'] for r in task_results]) * 100
            task_mae = np.mean([r['rollout_mae'] for r in task_results])
            task_mse = np.mean([r['rollout_mse'] for r in task_results])
            
            all_results[task_name] = {
                'num_samples': len(samples),
                'success_rate': float(task_success_rate),
                'average_mae': float(task_mae),
                'average_mse': float(task_mse),
                'rollouts': task_results
            }
            
            print(f"âœ… {task_name} Results (REAL DATA):")
            print(f"   Samples Found: {len(samples)}")
            print(f"   Success Rate: {task_success_rate:.1f}% ({sum([r['rollout_success'] for r in task_results])}/{len(samples)})")
            print(f"   Average MAE: {task_mae:.4f}")
            print(f"   Average MSE: {task_mse:.4f}")
            print(f"   Timesteps Processed: {len(samples) * timesteps_per_trial} ({timesteps_per_trial} per sample)")
            
            task_num += 1
        
        # Calculate overall metrics
        all_maes = []
        all_mses = []
        all_successes = []
        
        for task_name, task_data in all_results.items():
            all_maes.extend([r['rollout_mae'] for r in task_data['rollouts']])
            all_mses.extend([r['rollout_mse'] for r in task_data['rollouts']])
            all_successes.extend([r['rollout_success'] for r in task_data['rollouts']])
        
        overall_success_rate = np.mean(all_successes) * 100 if all_successes else 0.0
        overall_mae = np.mean(all_maes) if all_maes else 0.0
        overall_mse = np.mean(all_mses) if all_mses else 0.0
        
        print("\n" + "=" * 80)
        print("ðŸ“Š OVERALL RESULTS")
        print("=" * 80)
        print(f"Total Rollouts: {len(all_successes)}")
        print(f"Overall Success Rate: {overall_success_rate:.2f}%")
        print(f"Overall MAE: {overall_mae:.4f}")
        print(f"Overall MSE: {overall_mse:.4f}")
        print("=" * 80)
        
        # Save results
        output_file = f"sota_openvla_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w') as f:
            json.dump({
                'evaluation_config': {
                    'trials_per_task': trials_per_task,
                    'timesteps_per_trial': timesteps_per_trial,
                    'total_tasks': num_tasks,
                    'total_rollouts': len(all_successes)
                },
                'overall_metrics': {
                    'success_rate': float(overall_success_rate),
                    'mae': float(overall_mae),
                    'mse': float(overall_mse)
                },
                'task_results': all_results
            }, f, indent=2)
        
        print(f"\nðŸ’¾ Results saved to: {output_file}")
        return all_results

def main():
    parser = argparse.ArgumentParser(description='SOTA OpenVLA Replication on BridgeData V2')
    parser.add_argument('--model', type=str, default='openvla/openvla-7b', help='Model path')
    parser.add_argument('--trials', type=int, default=10, help='Trials per task')
    parser.add_argument('--timesteps', type=int, default=2, help='Timesteps per trial')
    parser.add_argument('--device', type=str, default='auto', choices=['auto', 'cpu', 'cuda', 'mps'])
    
    args = parser.parse_args()
    
    # Initialize replicator
    replicator = SOTAOpenVLAReplicator(model_path=args.model, device=args.device)
    
    # Load model
    if not replicator.load_model():
        print("âŒ Failed to load model. Exiting.")
        return
    
    # Run evaluation
    replicator.run_evaluation(trials_per_task=args.trials, timesteps_per_trial=args.timesteps)

if __name__ == "__main__":
    main()